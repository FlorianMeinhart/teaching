{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Arithmetic\n",
    "\n",
    "In this lecture, we discuss a little bit of convolution arithmetic, in particular, transposed convolutions. More details can be found [here](https://arxiv.org/pdf/1603.07285.pdf).\n",
    "\n",
    "\n",
    "| Symbol        | Meaning                                          |\n",
    "| ------------- |:------------------------------------------------:|\n",
    "| $s$           | Stride size (i.e., $s=1$)                        |\n",
    "| $k$           | Kernel size (i.e., $k \\times k$)                 |\n",
    "| $p$           | Padding size (i.e., $p=1$ on all sides)          |\n",
    "| $i$           | Spatial size of **input**  (e.g., $i \\times i$)  |\n",
    "| $o$           | Spatial size of **output**  (e.g., $o \\times o$) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining some random $i \\times i$ input and convolve this with a $k \\times k$ kernel using a stride of $s$ and no padding (i.e., $p=0$). *In all examples, we assume the same width and height of an input* (i.e., $i$ specifies both width and height).\n",
    "\n",
    "**Note**: **Relationship X** refers to [this](https://arxiv.org/pdf/1603.07285.pdf).\n",
    "\n",
    "### Convolution - NO PADDING, unit strides (i.e., $s=1$)\n",
    "\n",
    "Here's what we expect (**Relationship 1**)\n",
    "\n",
    "$$o = (i-k)+1$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4    # width=height=i=4\n",
    "s = 1    # stride=1\n",
    "k = 3    # kernel size 3x3\n",
    "p = 0    # NO PADDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size:                     torch.Size([1, 1, 2, 2])\n",
      "Expected (spatial) output size:  2\n"
     ]
    }
   ],
   "source": [
    "# Input\n",
    "x = torch.randn(1,1,i,i)\n",
    "# 2D conv. layer\n",
    "cl = nn.Conv2d(1,1,k,s)\n",
    "\n",
    "print('Output size:                    ', cl(x).size())\n",
    "print('Expected (spatial) output size: ', (i-k+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution -  Zero padding, unit strides\n",
    "\n",
    "Next, we check what happens in case of padding with zeros, in particular, padding by $1$ pixel on each side. We expect (**Relationship 2**)\n",
    "\n",
    "$$ o = (i-k)+2p+1$$\n",
    "\n",
    "This is, in fact, easy to see as we are effectively working with inputs of size $i+2p$; this relationship obviously holds for $p>1$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size:                     torch.Size([1, 1, 32, 32])\n",
      "Expected (spatial) output size:  32\n"
     ]
    }
   ],
   "source": [
    "i = 32    # width=height=i=5\n",
    "s = 1    # stride=1\n",
    "k = 3    # kernel size 3x3\n",
    "p = 1    # pad by one zero pixel on each side (i.e., bottom,top,right,left)\n",
    "\n",
    "x = torch.randn(1,1,i,i)\n",
    "cl = nn.Conv2d(1,1,k,s,padding=p)\n",
    "\n",
    "print('Output size:                    ', cl(x).size())\n",
    "print('Expected (spatial) output size: ', (i-k)+2*p+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution - NO PADDING, non-unit strides\n",
    "\n",
    "We expect (**Relationship 5**)\n",
    "\n",
    "$$ o = \\lfloor (i-k)/s \\rfloor +1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size:                     torch.Size([1, 1, 2, 2])\n",
      "Expected (spatial) output size:  2\n"
     ]
    }
   ],
   "source": [
    "i = 5    # width=height=i=5\n",
    "s = 2    # stride=2\n",
    "k = 3    # kernel size 3x3\n",
    "p = 0    # NO PADDING\n",
    "\n",
    "x = torch.randn(1,1,i,i)\n",
    "cl = nn.Conv2d(1,1,k,s,padding=p)\n",
    "\n",
    "print('Output size:                    ', cl(x).size())\n",
    "print('Expected (spatial) output size: ', np.floor((i-k)/s).astype(np.int)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution - Zero padding, non-unit strides\n",
    "\n",
    "We expect (**Relationship 6**), as we use inputs of effective size $i+2p$ (see **Relationship 2** from above), \n",
    "\n",
    "$$ o = \\lfloor (i+2p-k)/s \\rfloor +1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size:                     torch.Size([1, 1, 3, 3])\n",
      "Expected (spatial) output size:  3\n"
     ]
    }
   ],
   "source": [
    "i = 5    # width=height=i=5\n",
    "s = 2    # stride=2\n",
    "k = 3    # kernel size 3x3\n",
    "p = 1    # pad by one zero pixel on each side\n",
    "\n",
    "x = torch.randn(1,1,i,i)\n",
    "cl = nn.Conv2d(1,1,k,s,padding=p)\n",
    "\n",
    "print('Output size:                    ', cl(x).size())\n",
    "print('Expected (spatial) output size: ', np.floor((i+2*p-k)/s).astype(np.int)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we see an example, where the same convolution operation creates the same output size for different input sizes (only when $s>1$). This will lead to ambiguities when considering the corresponding operation in the opposite direction (i.e., *convolutional transpose* below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 3])\n",
      "torch.Size([1, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "i = 5    # INPUT0: width=height=i=5\n",
    "s = 2    # stride=2\n",
    "k = 3    # kernel size 3x3\n",
    "p = 1    # pad by one zero pixel on each side\n",
    "\n",
    "x = torch.randn(1,1,i,i)\n",
    "cl0 = nn.Conv2d(1,1,k,s,padding=p)\n",
    "print(cl0(x).size())\n",
    "\n",
    "i = 6    # INPUT1: width=height=i=6\n",
    "s = 2    # stride=2\n",
    "k = 3    # kernel size 3x3\n",
    "p = 1    # pad by one zero pixel on each side\n",
    "\n",
    "x = torch.randn(1,1,i,i)\n",
    "cl1 = nn.Conv2d(1,1,k,s,padding=p)\n",
    "print(cl1(x).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transposed (/Fractionally-strided) convolution\n",
    "\n",
    "To understand *transposed convolutions*, it is a good exercicse to study the standard convolution operation in greater detail, in particular, as a linear operation, taking, as input, the vectorized data and multiplying it with a suitably desigend linear operator (here $W$).\n",
    "\n",
    "In the following example, we set the input and weight of the 2D convolution (stride of 1, no padding, $3 \\times 3$ kernel) by hand to develop a little bit of intuition of what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the 2D conv. operation:\n",
      " tensor([[[[348., 393.],\n",
      "          [528., 573.]]]], grad_fn=<MkldnnConvolutionBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Input data, i=4\n",
    "x = torch.FloatTensor(np.array([\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8,],\n",
    "    [9,10,11,12],\n",
    "    [13,14,15,16]]))\n",
    "\n",
    "# Weights for our kxk, k=3, convolution kernel\n",
    "w = torch.FloatTensor(\n",
    "    np.array([\n",
    "        [1,2,3],\n",
    "        [4,5,6],\n",
    "        [7,8,9]]))\n",
    "\n",
    "# Add dimensions for PyTorch compatibility\n",
    "w = w.unsqueeze(0).unsqueeze(0)\n",
    "x = x.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "s = 1  # stride = 1\n",
    "k = 3  # kernel = 3x3\n",
    "\n",
    "cl4 = nn.Conv2d(1,1,k,s,bias=False)\n",
    "cl4.weight = nn.Parameter(w)\n",
    "out = cl4(x)\n",
    "print('Output of the 2D conv. operation:\\n', out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution operation (by hand):\n",
      " [[348 393]\n",
      " [528 573]]\n"
     ]
    }
   ],
   "source": [
    "# Lets perform the same operation by hand, see Sec. 4.1. in \n",
    "# https://arxiv.org/pdf/1603.07285.pdf\n",
    "\n",
    "# This is our linear operator C from above, C is (16x4)\n",
    "C = np.array([\n",
    "    [1,2,3,0,4,5,6,0,7,8,9,0,0,0,0,0],\n",
    "    [0,1,2,3,0,4,5,6,0,7,8,9,0,0,0,0],\n",
    "    [0,0,0,0,1,2,3,0,4,5,6,0,7,8,9,0],\n",
    "    [0,0,0,0,0,1,2,3,0,4,5,6,0,7,8,9],\n",
    "]).T\n",
    "\n",
    "# Our 4x4 input (will be vectorized to (1,16))\n",
    "X = np.array([\n",
    "    [1,2,3,4],\n",
    "    [5,6,7,8,],\n",
    "    [9,10,11,12],\n",
    "    [13,14,15,16]])\n",
    "\n",
    "# We compute y = x' * C, i.e., (1,16) x (16,4) = (1,4) -> reshape to 2x2\n",
    "print('Convolution operation (by hand):\\n', \n",
    "      np.dot(X.reshape((1,16)),C).reshape(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More details ...,\n",
    "\n",
    "Importantly, during backpropagation (through the convolution layer), the error signal (computed by the loss) is simply multiplied by $W^\\top$ (i.e., the transpose of the matrix $W$), as the partial derivative of $x^\\top W$ w.r.t. the input $x$ is $W^\\top$. With the 2D convolution from above, we have basically computed\n",
    "\n",
    "$$ y = x^\\top W $$\n",
    "\n",
    "resulting in a $1 \\times 4$ output $y$ which we reshaped into a $2 \\times 2$ matrix. Going the opposite \n",
    "direction (i.e., trying to reverse the operation), we see that \n",
    "\n",
    "$$ y W^\\top$$\n",
    "\n",
    "gives a $1 \\times 16$ output that can be reshaped into a $4 \\times 4$ matrix, i.e., our *original input shape*. This effectively explains the name *transposed* convolution. Lets look at some more interesting cases next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution - NO PADDING, unit strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2, 2])\n",
      "torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "i = 4    # width=height=i=4\n",
    "s = 1    # stride=1\n",
    "k = 3    # kernel size 3x3\n",
    "p = 0    # NO PADDING\n",
    "\n",
    "x = torch.randn(1,1,i,i)\n",
    "cl = nn.Conv2d(1,1,k,s,p)\n",
    "out = cl(x)\n",
    "print(out.size())\n",
    "\n",
    "# We could implement a transposed convolution in that case by\n",
    "# simply padding the input by 2 zeros on each side, using a 3x3\n",
    "# kernel and a stride of 1 to get a 4x4 output, as\n",
    "#\n",
    "# o = (i-k)+2p+1 = (2-3)+2*2+1 = 4\n",
    "tcl = nn.Conv2d(1,1,k,s,2)\n",
    "print(tcl(out).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets implement the same using PyTorch's `nn.ConvTranspose2d` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "i = 4    # width=height=i=4\n",
    "s = 1    # stride=1\n",
    "k = 3    # kernel size 3x3\n",
    "p = 0    # NO PADDING\n",
    "\n",
    "tcl = nn.ConvTranspose2d(1,1,k,s,p)\n",
    "print(tcl(out).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: Relationsip 8 in Sec. 4.3. would indicate setting `p=k-1` (so `p=3-1=2`), however, from the PyTorch documentation we see that the padding argument controls `kernel_size - 1 - padding`, so setting `p=0` is the appropriate choice here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution - Zero padding, unit strides\n",
    "\n",
    "So, the transposed convolution corresponding to a convolution with no padding and unit strides relies on zero padding the input by $k-1$ points. Similarly, if padding was used in the original convolution, this would imply a transposed convolution with *less* padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n",
      "torch.Size([1, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "i = 4    # width=height=i=4\n",
    "s = 1    # stride=1\n",
    "k = 3    # kernel size 3x3\n",
    "p = 1    # pad by 1 zero pixel on each side\n",
    "\n",
    "x = torch.randn(1,1,i,i)\n",
    "cl = nn.Conv2d(1,1,k,s,p)\n",
    "out = cl(x)\n",
    "print(out.size())\n",
    "\n",
    "tcl = nn.ConvTranspose2d(1,1,k,s,p) # here we now have k-1-p with p=1, effectively\n",
    "print(tcl(out).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution - NO PADDING, non-unit strides\n",
    "\n",
    "Here, we assume, $i-k$ is a multiple of $s$, in other words, the valid convolution (operating at stride $s$) covers all the input pixel. Otherwise, we would map multiple input shapes to the same output shape. Just consider $i=\\{5,6\\},k=3,s=2$. Both, inputs of size $5 \\times 5$ and $6 \\times 6$ would be mapped to a $2 \\times 2$ output (we have seen this earlier). We resolve this ambiguity later ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2, 2])\n",
      "torch.Size([1, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "i = 6    # width=height=i=5\n",
    "s = 2    # stride=1\n",
    "k = 3    # kernel size 3x3\n",
    "p = 0\n",
    "\n",
    "# Lets check: i-k is a multiple of s\n",
    "# 5-3 = 2\n",
    "# 7-3 = 4\n",
    "# 6-3 = 3 - so 3 mod 2 = 1 - THIS DOES NOT WORK!\n",
    "\n",
    "# 2D CONV\n",
    "x = torch.randn(1,1,i,i)\n",
    "cl = nn.Conv2d(1,1,k,s,p)\n",
    "out = cl(x)\n",
    "print(out.size())\n",
    "\n",
    "# corresponding 2D CONV TRANSP.\n",
    "tcl = nn.ConvTranspose2d(1,1,k,s,p)\n",
    "print(tcl(out).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposed Convolution - Zero padding, non-unit strides\n",
    "\n",
    "Here, we assume, $i-2p+k$ is a multiple of $s$ (otherwise, we have the same problem as mentioned above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 5])\n",
      "torch.Size([1, 1, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "i = 9   # width=height=i=9\n",
    "s = 2    # stride=1\n",
    "k = 3    # kernel size 3x3\n",
    "p = 1    # pad by one zero pixel on each side\n",
    "\n",
    "# for i = 9,11,..., we have i-2*p+k are multiples of s\n",
    "# 9  - 2 + 3  = 10\n",
    "# 11 - 2 + 3  = 12\n",
    "\n",
    "# 2D CONV\n",
    "x = torch.randn(1,1,i,i)\n",
    "cl = nn.Conv2d(1,1,k,s,p)\n",
    "out = cl(x)\n",
    "print(out.size())\n",
    "\n",
    "# corresponding 2D CONV TRANSP.\n",
    "tcl = nn.ConvTranspose2d(1,1,k,s,p)\n",
    "print(tcl(out).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve the ambiguity mentioned above, we have the possiblity to use the `output_padding` ($a$) parameter. This parameter, computed as \n",
    "\n",
    "$$ a = (i+2p-k)\\mod 2$$\n",
    "\n",
    "essentially takes care of resolving the ambiguity by adding padding zeros to 2 sides of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 5])\n",
      "Output padding a=1\n",
      "torch.Size([1, 1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "i = 10   # width=height=i=10\n",
    "s = 2    # stride=2\n",
    "k = 3    # kernel size 3x3\n",
    "p = 1    # pad by one zero pixel on each side\n",
    "\n",
    "# i - 2*p + k = 23 - 2 + 2 = 23 NOT a multiple of s\n",
    "\n",
    "# 2D CONV\n",
    "x = torch.randn(1,1,i,i)\n",
    "cl = nn.Conv2d(1,1,k,s,p)\n",
    "out = cl(x)\n",
    "print(out.size())\n",
    "\n",
    "# Compute a\n",
    "a = np.mod(i+2*p-k,s)\n",
    "print('Output padding a={}'.format(a))\n",
    "\n",
    "# corresponding 2D CONV TRANSP.\n",
    "tcl = nn.ConvTranspose2d(1,1,k,s,p,output_padding=a)\n",
    "print(tcl(out).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
