{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 (15 points)\n",
    "(due on Nov. 25, 23:59pm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third assignment, we will write a Convolutional Neural Network (CNN) together with training and evaluation routines.\n",
    "\n",
    "The task description can be found [below](#Task).\n",
    "\n",
    "**Important**: I strongly recommend to use *Google Collab (GC)* for this assignment. Make yourself familiar with running Jupyter notebooks on GC (especially selecting the right runtime, i.e., Python 3 + GPU). This will make your life a lot easier, as training will be faster and you can easily debug problems in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "torch.manual_seed(1234);\n",
    "np.random.seed(1234);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))])\n",
    "         \n",
    "ds_train = CIFAR10('/tmp/cifar', \n",
    "                 train=True, \n",
    "                 transform=transforms.ToTensor(), \n",
    "                 target_transform=None, \n",
    "                 download=True)\n",
    "\n",
    "ds_test = CIFAR10('/tmp/cifar', \n",
    "                 train=False, \n",
    "                 transform=transforms.ToTensor(), \n",
    "                 target_transform=None, \n",
    "                 download=True)\n",
    "\n",
    "lab = [ds_train[x][1] for x in range(len(ds_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_indices(n_splits, train_size, lab):\n",
    "    s = StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, \n",
    "        train_size=train_size, \n",
    "        test_size=None)\n",
    "    \n",
    "    return [i.tolist() for i, _ in s.split(lab, lab)]\n",
    "\n",
    "train_indices = generate_train_indices(10, 500, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', \n",
    "           'car', \n",
    "           'bird', \n",
    "           'cat',\n",
    "           'deer', \n",
    "           'dog', \n",
    "           'frog', \n",
    "           'horse', \n",
    "           'ship', \n",
    "           'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_subset = Subset(ds_train, train_indices[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter([ds_train_subset[i][1] for i in range(len(ds_train_subset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(ds: torchvision.datasets.cifar.CIFAR10, \n",
    "                indices: list):\n",
    "    \n",
    "    assert np.max(indices) < len(ds)\n",
    "    \n",
    "    plt.figure(figsize=(9, len(indices)));\n",
    "    for j,idx in enumerate(indices):\n",
    "        plt.subplot(1,len(indices),j+1)\n",
    "        plt.imshow(ds[idx][0].permute(1,2,0).numpy())\n",
    "        plt.title('Label={}'.format(classes[ds[idx][1]]),fontsize=9)\n",
    "\n",
    "show_images(ds_train_subset, [1,3,499,2,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(net):\n",
    "    num_param = 0\n",
    "    for p in net.parameters():\n",
    "        num_param += p.numel()\n",
    "    return num_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "The assignment is split into 4 parts: (1) writing the model definition, (2) writing the training code, (3) writing the testing code and (4) writing the *glue* code which initializes the model, sets the optimizer (and scheduler) and eventually performs a couple of epochs of training + testing.\n",
    "\n",
    "*First*, implement the following **convolutional neural network (CNN)**: It consists of 3 blocks and a simple linear classifier at the end.\n",
    "\n",
    "My notation denotes the following:\n",
    "\n",
    "- `Conv2D(in_channels, out_channels, kernel_size, padding)` - 2D Convolution\n",
    "- `MaxPool(kernel_size, stride, padding)` - Max. pooling\n",
    "- `AvgPool(kernel_size, stride, padding)` - Avg. pooling\n",
    "- `Dropout(dropout_probability)` - Dropout layer\n",
    "- `BatchNorm2D` - 2D batch normalization\n",
    "\n",
    "All these operations can also be found in the [PyTorch documentaton](https://pytorch.org/docs/stable/index.html).\n",
    "\n",
    "**Block 1**\n",
    "\n",
    "```\n",
    "Conv2D(  3,128,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n",
    "Conv2D(128,128,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n",
    "Conv2D(128,128,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n",
    "MaxPool(2,2,0)\n",
    "Dropout(0.5)\n",
    "```\n",
    "The output size at that point should be $N \\times 128 \\times 16 \\times 16$.\n",
    "\n",
    "**Block 2**\n",
    "\n",
    "```\n",
    "Conv2D(128,256,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n",
    "Conv2D(256,256,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n",
    "Conv2D(256,256,3,1) -> Batchnorm2D -> LeakyReLU(0.1)\n",
    "MaxPool(2,2,0)\n",
    "Dropout(0.5)\n",
    "```\n",
    "The output size at that point should be $N \\times 128 \\times 8 \\times 8$.\n",
    "\n",
    "**Block 3**\n",
    "\n",
    "```\n",
    "Conv2D(256,512,3,0) -> Batchnorm2D -> LeakyReLU(0.1)\n",
    "Conv2D(512,256,1,0) -> Batchnorm2D -> LeakyReLU(0.1)\n",
    "Conv2D(256,128,1,0) -> Batchnorm2D -> LeakyReLU(0.1)\n",
    "AvgPool(6,2,0)\n",
    "Dropout(0.5)\n",
    "```\n",
    "The output size at that point should be $N \\times 128 \\times 1 \\times 1$.\n",
    "\n",
    "**Classifier**\n",
    "\n",
    "View the output of the last block as a $1 \\times 128$ tensor and add a \n",
    "linear layer mapping from $\\mathbb{R}^{128} \\rightarrow \\mathbb{R}^{10}$\n",
    "(include bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module): \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        def make_block(conv_config, pooling_op=None, use_dropout=False):\n",
    "            mlist = nn.ModuleList()\n",
    "            for in_c, out_c, k_size, pad in conv_config:\n",
    "                mlist.extend([\n",
    "                    nn.Conv2d(in_c, out_c, k_size, padding=pad),\n",
    "                    nn.BatchNorm2d(out_c),\n",
    "                    nn.LeakyReLU(0.1)\n",
    "                ])\n",
    "            mlist.append(pooling_op)\n",
    "            if use_dropout:\n",
    "                mlist.append(nn.Dropout(0.5))\n",
    "            return mlist\n",
    "\n",
    "        self.block1 = make_block([\n",
    "            [  3,128,3,1],\n",
    "            [128,128,3,1],\n",
    "            [128,128,3,1]], \n",
    "            nn.MaxPool2d(2,stride=2,padding=0), \n",
    "            use_dropout=True)\n",
    "        \n",
    "        self.block2 = make_block([\n",
    "            [128,256,3,1],\n",
    "            [256,256,3,1],\n",
    "            [256,256,3,1]], \n",
    "            nn.MaxPool2d(2,stride=2,padding=0),\n",
    "            use_dropout=True)\n",
    "\n",
    "        self.block3 = make_block([\n",
    "            [256,512,3,0],\n",
    "            [512,256,1,0],\n",
    "            [256,128,1,0]], \n",
    "            nn.AvgPool2d(6,stride=2,padding=0),\n",
    "            use_dropout=False)\n",
    "        \n",
    "        self.classifier = nn.Linear(128,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for l in self.block1: x = l(x)\n",
    "        for l in self.block2: x = l(x)\n",
    "        for l in self.block3: x = l(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ConvNet(10)\n",
    "out = net(torch.rand(5,3,32,32))\n",
    "print(out.size())\n",
    "print(count_params(net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second**, write a training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third**, write a testing routine ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**, write the *glue* code which performs a couple of epochs (e.g., 100) of training and, after each epoch, test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    ds_train_subset,\n",
    "    batch_size=32,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    ds_test, \n",
    "    batch_size=64, \n",
    "    shuffle=False)\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=0.01, \n",
    "    momentum=0.9)\n",
    "\n",
    "for epoch in range(1,10+1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
