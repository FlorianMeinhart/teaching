{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation (in PyTorch)\n",
    "\n",
    "**Course**: Computer Vision (911.908)    \n",
    "**Author**: Roland Kwitt (Dept. of Computer Science, Univ. of Salzburg)       \n",
    "Winter term 2019/20\n",
    "\n",
    "\n",
    "In this lecture, we take a look at the mechanics of **automatic differentiation** (aka *AutoGrad* / *AutoDiff*) in PyTorch. These techniques are at the very heart of today's frameworks for learning with neural networks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [Introductory example](#Introductory-example)\n",
    "- [A  slightly more involved example](#A-slightly-more-involved-example)\n",
    "- [The AutoGrad machinery](#The-AutoGrad-machinery)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import agtree2dot\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils import visualize_DAG\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introductory example\n",
    "\n",
    "In this example, we have a following computation graph:\n",
    "\n",
    "<img src=\"DAG0.svg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compute  \n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial b} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial b}$$\n",
    "\n",
    "First, \n",
    "\n",
    "$$ \\frac{\\partial v}{\\partial b} = 1 $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial a}{\\partial v} = \\frac{\\partial}{\\partial v} \\text{ReLU}(v) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if}~v \\leq 0\\\\\n",
    "1 & \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Hence, in our case we have\n",
    "\n",
    "$$\\frac{\\partial a}{\\partial v} = 1$$\n",
    "\n",
    "and consequently \n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial b} = 1\n",
    "$$\n",
    "\n",
    "Let's try to compute the partial derivative of $a$ wrt. $w$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial u}\\frac{\\partial u}{\\partial w}\n",
    "$$\n",
    "\n",
    "So, \n",
    "\n",
    "$$\n",
    "\\frac{\\partial v}{\\partial u} = 1\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial w} = x\n",
    "$$\n",
    "\n",
    "Combined, we obtain (if we use the fact that $x=3$)\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v}\\frac{\\partial v}{\\partial u}\\frac{\\partial u}{\\partial w} = 1\\cdot 1\\cdot 3 = 3\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Conceptually, the **forward pass** (i.e., the computation of the function value) is a standard tensor computation (i.e., in this example simply the computation of all intermediate results), and the **Directed Ascyclic Graph (DAG)** of tensor operations is required only to compute derivatives (via the chain rule). \n",
    "\n",
    "When executing tensor operations, PyTorch can automatically (on-the-fly) construct the graph of operations to compute the gradient of any quantity\n",
    "with respect to any tensor involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([3.])\n",
    "w = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "u = x*w\n",
    "v = u+b\n",
    "a = torch.clamp(v, min=0) # alternatively: a = F.relu(x*w + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensor has a Boolean field `requires_grad`, set to `False` by default, which\n",
    "states if PyTorch should build the graph of operations so that gradients with\n",
    "respect to it can be computed.\n",
    "\n",
    "**Note**: Only *floating point type* tensors can have their gradient computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"v =\", v.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient $\\partial a/\\partial w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_w = grad(a, w, retain_graph=True)\n",
    "print(grad_w[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gradient $\\partial a/\\partial b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad(a, b)[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A slightly more involved example\n",
    "\n",
    "In this example, we have a slightly more complex computation graph with a *shared* weight $w$ and two paths to arrive at the result (fix: pow(.,3))\n",
    "\n",
    "<img src=\"DAG1.svg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the partial derivative of $a$ wrt. $w$ by hand once more:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial a}{\\partial w} = \\frac{\\partial a}{\\partial v_1}\\frac{\\partial v_1}{\\partial u_1}\\frac{\\partial u_1}{\\partial w} + \\frac{\\partial a}{\\partial v_2}\\frac{\\partial v_2}{\\partial u_2}\\frac{\\partial u_2}{\\partial w} = \n",
    "1\\cdot 2u_1\\cdot x + 1 \\cdot 3u_2^2 \\cdot x = 8 + 24 = 32\n",
    "$$\n",
    "\n",
    "Again, having **all the intermediate values** during the **forward pass** makes it very easy to actually compute the gradient of $a$ wrt. $w$, i.e., $\\partial a/\\partial w$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([2.])\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "u1 = x*w\n",
    "u2 = x*w\n",
    "\n",
    "v1 = torch.pow(u1, 2)\n",
    "v2 = torch.pow(u2, 3)\n",
    "\n",
    "a = v1 + v2\n",
    "print(\"a =\", a.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the gradient of $a$ wrt. $w$, i.e., $\\partial a/\\partial w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad(a, w)[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The AutoGrad machinery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autograd DAG is encoded through the fields `grad_fn` of Tensors, and the\n",
    "fields `next_functions` of Functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# below is another way of saying requires_grad=True\n",
    "x = torch.tensor([ 1.0, -2.0, 3.0, -4.0 ]).requires_grad_() \n",
    "a = x.abs()\n",
    "s = a.sum()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s.grad_fn.next_functions)\n",
    "print(s.grad_fn.next_functions[0][0].next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we visualize a simple computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 2.]).requires_grad_()\n",
    "q = x.norm() # l2-norm of tensor sqrt(1+4+4)=9\n",
    "print(q.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agtree2dot.save_dot(q, {x: 'x', q: 'q'}, open('simple1.dot', 'w'))\n",
    "HTML(visualize_DAG('simple1.dot', 'simple1.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another, slightly more involved, example: We have $\\mathbf{x} \\in \\mathbb{R}^{10}$ and **first** compute\n",
    "\n",
    "$$\n",
    "\\mathbf{h}  = \\text{tanh}(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) \\\\\n",
    "$$\n",
    "\n",
    "with $\\mathbf{W}_1 \\in \\mathbb{R}^{20 \\times 10}$ and $\\mathbf{b}_1 \\in \\mathbb{R}^{10}$. This results in $\\mathbf{h} \\in \\mathbb{R}^{20}$. **Second**, we compute\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}}  = \\text{tanh}(\\mathbf{W}_2\\mathbf{h} + \\mathbf{b}_2)\n",
    "$$\n",
    "\n",
    "with $\\mathbf{W}_2 \\in \\mathbb{R}^{20 \\times 5}$ and $\\mathbf{b}_2 \\in \\mathbb{R}^5$. This results in \n",
    "$\\hat{\\mathbf{y}} \\in \\mathbb{R}^5$. And **finally**, we compute \n",
    "\n",
    "$$\n",
    "L(\\mathbf{y},\\hat{\\mathbf{y}}) = \\frac{1}{5} \\sum_{i=1}^5(y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "where $y_i,\\hat{y}_i$ are the $i$-th elements of $\\mathbf{y}$ and $\\hat{\\mathbf{y}}$, respectively. In the example below, $y_i$ (which we use as targets) are just random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.rand(20, 10).requires_grad_()\n",
    "b1 = torch.rand(20).requires_grad_()\n",
    "w2 = torch.rand(5, 20).requires_grad_()\n",
    "b2 = torch.rand(5).requires_grad_()\n",
    "\n",
    "x = torch.rand(10)\n",
    "h = torch.tanh(w1 @ x + b1)\n",
    "y = torch.tanh(w2 @ h + b2)\n",
    "\n",
    "target = torch.rand(5)\n",
    "\n",
    "loss = (y - target).pow(2).mean()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the computation graph ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agtree2dot.save_dot(loss, \n",
    "                    {\n",
    "                        w1: 'w1',\n",
    "                        b1: 'b1',\n",
    "                        w2: 'w2',\n",
    "                        b2: 'b2',\n",
    "                        loss: 'loss'\n",
    "                    }, open('simple2.dot', 'w'))\n",
    "HTML(visualize_DAG('simple2.dot', 'simple2.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `torch.no_grad()`\n",
    "\n",
    "The `torch.no_grad()` context switches off the autograd machinery, and can be\n",
    "used for operations such as parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor( 0.5).requires_grad_()\n",
    "b = torch.tensor(-0.5).requires_grad_()\n",
    "eta = 0.1\n",
    "\n",
    "for k in range(100):\n",
    "    l = (a - 1)**2 + (b + 1)**2 + (a - b)**2\n",
    "    ga, gb = torch.autograd.grad(l, (a, b))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= eta * ga\n",
    "        b -= eta * gb\n",
    "        \n",
    "print('%.06f' % a.item(), '%.06f' % b.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `detach()`\n",
    "\n",
    "The `detach()` method creates a tensor which shares the data, but does not\n",
    "require gradient computation, and is not connected to the current graph.\n",
    "\n",
    "This method should be used when the gradient should not be propagated\n",
    "beyond a variable, or to update leaf tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor( 0.5).requires_grad_()\n",
    "b = torch.tensor(-0.5).requires_grad_()\n",
    "eta = 0.1\n",
    "\n",
    "for k in range(100):\n",
    "    l = (a - 1)**2 + (b + 1)**2 + (a.detach() - b)**2\n",
    "    ga, gb = torch.autograd.grad(l, (a, b))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a -= eta * ga\n",
    "        b -= eta * gb\n",
    "        \n",
    "print('%.06f' % a.item(), '%.06f' % b.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
